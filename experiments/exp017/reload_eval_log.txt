
-------------------- Extract model info from the train_log.txt -------------------
Class of the model: CNNLeNet
Quantised model: True
Number of classes: 10
(num_classes = 10)
Test set path: data/nmnist/Plain_Binary/Plain-Binary_1FramePerEventSet_test_dataset.pth
Scale: 0.007870171219110489
Zero point: 0

-------------------- Model info extracted successfully -------------------

------------------- Model initialisation and loading -------------------
Model was quantised, model structure: 
CNNLeNet_q(
  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)
  (dequant): DeQuantize()
  (conv1): QuantizedConv2d(2, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.10679341107606888, zero_point=76, bias=False)
  (relu1): ReLU()
  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (conv2): QuantizedConv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.261362224817276, zero_point=79, bias=False)
  (relu2): ReLU()
  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)
  (conv3): QuantizedConv2d(16, 120, kernel_size=(5, 5), stride=(1, 1), scale=0.3414236903190613, zero_point=79, bias=False)
  (relu3): ReLU()
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (fc1): QuantizedLinear(in_features=120, out_features=84, scale=0.3348349928855896, zero_point=66, qscheme=torch.per_channel_affine)
  (relu4): ReLU()
  (fc2): QuantizedLinear(in_features=84, out_features=10, scale=0.7119020223617554, zero_point=78, qscheme=torch.per_channel_affine)
)
Load model from: experiments/exp017/quantised_model.pth

------------------- Model successfully initialised ------------------- 

------------------------------- Test Data loading -------------------------------
Test set path: data/nmnist/Plain_Binary/Plain-Binary_1FramePerEventSet_test_dataset.pth
Total number of samples in test_loader: 10000

---------------------------- Test Data loaded successfully ----------------------------

######################################### Classification report #########################################
              precision    recall  f1-score   support

           0     0.9838    0.9939    0.9888       980
           1     0.9912    0.9956    0.9934      1135
           2     0.9827    0.9913    0.9870      1032
           3     0.9823    0.9881    0.9852      1010
           4     0.9869    0.9939    0.9904       982
           5     0.9809    0.9798    0.9804       892
           6     0.9894    0.9770    0.9832       958
           7     0.9912    0.9874    0.9893      1028
           8     0.9805    0.9795    0.9800       974
           9     0.9909    0.9722    0.9815      1009

    accuracy                         0.9861     10000
   macro avg     0.9860    0.9859    0.9859     10000
weighted avg     0.9861    0.9861    0.9861     10000


##########################################################################################################

############################################# Accuracy score #############################################
0.9861

##########################################################################################################

##################### [Inference time] - Testing model for 5 iterations #####################

### Testing - Iteration 1/5 ###


Iteration 1 completed: Total examples: 10000, Accuracy: 98.61%
Inference time for iteration 1: 0 min 9.13 sec

### Testing - Iteration 2/5 ###


Iteration 2 completed: Total examples: 10000, Accuracy: 98.61%
Inference time for iteration 2: 0 min 9.14 sec

### Testing - Iteration 3/5 ###


Iteration 3 completed: Total examples: 10000, Accuracy: 98.61%
Inference time for iteration 3: 0 min 9.18 sec

### Testing - Iteration 4/5 ###


Iteration 4 completed: Total examples: 10000, Accuracy: 98.61%
Inference time for iteration 4: 0 min 9.16 sec

### Testing - Iteration 5/5 ###


Iteration 5 completed: Total examples: 10000, Accuracy: 98.61%
Inference time for iteration 5: 0 min 9.33 sec

Average Inference time over 5 iterations: 0 min 9.19 sec

Average Inference time per sample: 0.92 ms

##################### [Inference time] - Testing completed #####################
